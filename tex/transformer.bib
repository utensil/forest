# A few hints:
#
# To normalize:
# # bibtool --preserve.key.case=on --preserve.keys=on --pass.comments=on --print.use.tab=off -s -i content/posts/transformer/bib.bib -o content/posts/transformer/bib.bib -- 'new.entry.type {thesis}' -- 'new.entry.type {online}'
# To convert to JSON:
# pandoc content/posts/transformer/bib.bib -t csljson -o content/posts/transformer/bib.json

@Article{         ba2016layer,
  title         = {Layer normalization},
  author        = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey
                  E},
  journal       = {arXiv preprint arXiv:1607.06450},
  year          = {2016},
  url           = {https://arxiv.org/abs/1607.06450}
}

@Article{         bahdanau2014neural,
  title         = {Neural machine translation by jointly learning to align
                  and translate},
  author        = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal       = {arXiv preprint arXiv:1409.0473},
  year          = {2014},
  url           = {https://arxiv.org/abs/1409.0473}
}

@Article{         cheng2016long,
  title         = {Long short-term memory-networks for machine reading},
  author        = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
  journal       = {arXiv preprint arXiv:1601.06733},
  year          = {2016},
  url           = {https://arxiv.org/abs/1601.06733}
}

@Article{         chiang2021named,
  title         = {Named tensor notation},
  author        = {Chiang, David and Rush, Alexander M and Barak, Boaz},
  journal       = {arXiv preprint arXiv:2102.13196},
  year          = {2021},
  url           = {https://arxiv.org/abs/2102.13196}
}

@Article{         liu2018generating,
  title         = {Generating wikipedia by summarizing long sequences},
  author        = {Liu, Peter J and Saleh, Mohammad and Pot, Etienne and
                  Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and
                  Shazeer, Noam},
  journal       = {arXiv preprint arXiv:1801.10198},
  year          = {2018},
  url           = {https://arxiv.org/abs/1801.10198}
}

@Article{         phuong2022formal,
  title         = {Formal algorithms for transformers},
  author        = {Phuong, Mary and Hutter, Marcus},
  journal       = {arXiv preprint arXiv:2207.09238},
  year          = {2022},
  url           = {http://arxiv.org/abs/2207.09238}
}

@Article{         press2021train,
  title         = {Train short, test long: Attention with linear biases
                  enables input length extrapolation},
  author        = {Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal       = {arXiv preprint arXiv:2108.12409},
  year          = {2021},
  url           = {https://arxiv.org/abs/2108.12409}
}

@InProceedings{   rogozhnikov2021einops,
  title         = {Einops: Clear and reliable tensor manipulations with
                  einstein-like notation},
  author        = {Rogozhnikov, Alex},
  booktitle     = {International Conference on Learning Representations},
  year          = {2021},
  url           = {https://openreview.net/forum?id=oapKSVM2bcj}
}

@InProceedings{   szegedy2017inception,
  title         = {Inception-v4, inception-resnet and the impact of residual
                  connections on learning},
  author        = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke,
                  Vincent and Alemi, Alexander},
  booktitle     = {Proceedings of the AAAI conference on artificial
                  intelligence},
  volume        = {31},
  number        = {1},
  year          = {2017},
  url           = {https://arxiv.org/abs/1602.07261}
}

@Article{         turner2023introduction,
  title         = {An introduction to transformers},
  author        = {Turner, Richard E},
  journal       = {arXiv preprint arXiv:2304.10557},
  year          = {2023},
  url           = {http://arxiv.org/abs/2304.10557}
}

@Article{         vaswani2017attention,
  title         = {Attention is all you need},
  author        = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                  Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                  Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal       = {Advances in neural information processing systems},
  volume        = {30},
  year          = {2017},
  url           = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@Misc{          weng2023transformer,
  author        = {Weng, Lilian},
  title         = {The Transformer Family Version 2.0},
  url           = {https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/},
  year          = {2023}
}
