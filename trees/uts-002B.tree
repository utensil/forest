\import{macros}

\mdnote{ML}{
I don't want efforts in [Transformers: from self-attention to performance optimizations](https://utensil.github.io/blog/posts/transformer/) to be discontinued, lately there is \citek{ferrando2024primer} on this topic.

I might need to follow on the latest development on the linear attention mechanism \citek{peng2024eagle}.

I have almost no understanding of diffusion models, so I should read \citek{bao2023all} and related papers.

I should also read \citek{mikula2023magnushammer} and related papers.
}
